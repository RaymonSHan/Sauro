
* BROWSER
use py
http://scrapy.org/download/
http://www.spiderfoot.net/download/


top 50 list
http://bigdata-madesimple.com/top-50-open-source-web-crawlers-for-data-mining/

36 crawler
http://www.findbestopensource.com/tagged/webcrawler

| name           | url                                | desc                                     |
|----------------+------------------------------------+------------------------------------------|
| GNU Wget       | https://www.gnu.org/software/wget/ | too old, no desc to javascript           |
| DataparkSearch | http://www.dataparksearch.org/     | including some JavaScript link decoding. |
| HTTrack        | http://www.httrack.com/            | it more look like a reocrder, not spider |
|                |                                    |                                          |

all page have said, should do program for ajax POST in Scrapy, the most recommend sprider now.
http://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax


v8
http://izs.me/v8-docs/v8_8h.html


useful 
http://my.oschina.net/u/1024140/blog/119752  !!
    https://github.com/scrapinghub
http://www.cnblogs.com/MingZznet/articles/3210052.html
http://qinxuye.me/article/cola-a-distributed-crawler-framework/ cola
http://blog.dutchcoders.io/using-scrapy-and-pyv8-to-scrape-inline-javascript/ v8 pyv8
https://cnodejs.org/topic/51b13d7d555d34c678fb3e88 nodejs mongoDB


http://codemirror.net/1/ unknown yet

http://www.jb51.net/article/57183.htm very beginning
http://blog.csdn.net/zzllabcd/article/details/21398163 install step
http://blog.csdn.net/gugugujiawei/article/details/42968745 good step


http://splash.readthedocs.org/en/latest/scripting-tutorial.html
goin

* DONE INSTALL SprapingHub
  CLOSED: [2015-08-27 Thu 14:48]
TOTAL: https://github.com/scrapinghub 
USE: scrapy
USE: splash

** INSTALL scrapy
scrapy:  https://github.com/scrapinghub/scrapy
REQ: python v2.7
RUN: sudo pip install scrapy
DOC: http://doc.scrapy.org/en/latest/

** INSTALL pip
error for http://blog.csdn.net/henulwj/article/details/48131393  add in Oct. 14 '15

** INSTALL docker
docker request by splash : http://docs.docker.com/linux/step_one/
RUN: sudo wget -qO- https://get.docker.com/ | sh

** INSTALL splash
splash: http://splash.readthedocs.org/en/latest/install.html
REQ: docker
RUN: sudo docker pull scrapinghub/splash
DOC: http://splash.readthedocs.org/en/latest/scripting-tutorial.html

** RUN splash
splash
RUN: sudo docker run -p 8050:8050 scrapinghub/splash
TRY: curl 'http://127.0.0.1:8050/execute?lua_source=function+main%28splash%29%0D%0A++return+%27hello%27%0D%0Aend' 
    RETURN:hello

** INSTALL scrapyjs
scrapyjs: https://github.com/scrapinghub/scrapy-splash
RUN: sudo pip install scrapyjs
MODI: settings.py: 

* DONE LEARN Lua basic
  CLOSED: [2015-08-27 Thu 16:09]
Lua: http://tylerneylon.com/a/learn-lua/
| Lua                                             | C++                         | equ / comment                     |
|-------------------------------------------------+-----------------------------+-----------------------------------|
| --                                              | //                          |                                   |
| --[[ comment --] ]                              | /* comment */               | ] ] should be ]]                  |
| num = 42                                        | double num = 42;            | 64bit, 52bit for base             |
| s = 'immutable'                                 | const char *s ="immutable"; |                                   |
| s = [[ multi                                    | char *s =" multi \          |                                   |
| ...... line ] ]                                 | ...... line ";              | ] ] should be ]]                  |
| t = nil                                         |                             | for garbage                       |
|-------------------------------------------------+-----------------------------+-----------------------------------|
| while a<10 do a=a+1 end                         |                             |                                   |
| if then elseif then else end                    |                             |                                   |
| for i = 100, 1, -1 do j = j + 1 end             |                             |                                   |
| repeat i = i-1 until i = 0                      |                             |                                   |
| if not i ~= 1 then end                          | ~= !=                       |                                   |
| foo = unDefine                                  |                             | foo = nil                         |
|-------------------------------------------------+-----------------------------+-----------------------------------|
| function foo(n) return n end                    |                             |                                   |
| function f(x) return x * x end                  |                             | f = function (x) return x * x end |
| print 'hello'                                   |                             | print ('hello')                   |
| for key, val in pairs(u) do print(key, val) end |                             | in, pairs are keywords            |
| for i = 1, #v do print(v[i]) end                | #v sizeof(v)                |                                   |
| print('I say ' .. self.sound)                   | .. strcat()                 |                                   |

* DONE LEARN splash tutorial
  CLOSED: [2015-08-27 Thu 18:56]
splash : http://splash.readthedocs.org/en/latest/scripting-tutorial.html
Basic UI : http://127.0.0.1:8050/
Author's reason : https://www.reddit.com/r/Python/comments/2xp5mr/handling_javascript_in_scrapy_with_splash/

* DONE TRY splash doing.
  CLOSED: [2015-08-28 Fri 17:05]
the way use js for href : http://jingyan.baidu.com/article/c45ad29cf6c075051653e24d.html

the way to run js and get result
curl -X POST -H 'content-type: application/json' \
    -d '{"js_source": "javascript:go(curPage+1);return false;", "url": "http://stock.sohu.com/stock_scrollnews_25.shtml", "script": "1"}' \
    'http://localhost:8050/render.json'

curl -X POST -H 'content-type: application/json' \
    -d '{"js_source": "function getContents(){ go(curPage+1); return document.location.href; }; getContents();", "url": "http://stock.sohu.com/stock_scrollnews_25.shtml", "script": "1"}' \
    'http://localhost:8050/render.json'

curl -X POST -H 'content-type: application/javascript' \
    -d 'function getContents(){ go(curPage+1); return document.location.href; }; getContents();' \
    'http://localhost:8050/render.json?url=http://stock.sohu.com/stock_scrollnews_25.shtml&script=1'

curl -X POST -H 'content-type: application/json' \
    -d '{"lua_source": "function main(splash) splash:go(\"http://stock.sohu.com/stock_scrollnews_25.shtml\") local title=splash:runjs(\"javascript:go(curPage+1);\") splash:wait(1) local href=splash:evaljs(\"document.location.href;\") return {title=title, href=href} end" }' \
    'http://localhost:8050/execute'

all the four are rational, but should sleep after run the js. there are no sleep in js, only in lua.
And 0.2s is not enough, at least 0.3s in my T420. or it return the preview href before js run.

curl -X POST -H 'content-type: application/json' \
    -d '{"lua_source": "function main(splash) splash:go(\"http://stock.sohu.com/stock_scrollnews_25.shtml\") local title=splash:runjs(\"javascript:go(curPage+1);return false;\") splash:wait(1) local href=splash:evaljs(\"document.location.href;\") return {title=title, href=href} end" }' \
    'http://localhost:8050/execute'

but the real js, with "return false" addition, return bad. I know the reason why js in sohu seem so strange, everything is trap for splash.
so splash may be pause.

curl -X POST -H 'content-type: application/json' \
    -d '{"lua_source": "function main(splash) local sohufunc = splash:jsfunc([[ function (){ go(curPage+1); return false; } ]]) local getfunc = splash:jsfunc([[ function (){ var ishref = document.location.href; return ishref;} ]]) splash:go(\"http://stock.sohu.com/stock_scrollnews_125.shtml\") sohuf=sohufunc() splash:wait(1) getf=getfunc() return {sohuf=sohuf, getfc=getf} end" }' \
    'http://localhost:8050/execute'

only this is OK

* TODO LEARN python DOING ex40
python: http://learnpythonthehardway.org/book/
put in top of .py for encode : (in ex1) -*- coding: utf-8 -*-
| python                                                                  | C++    | comment                                         |
|-------------------------------------------------------------------------+--------+-------------------------------------------------|
| #                                                                       | //     |                                                 |
| print "He's got %s eyes and %s hair." % (my_eyes, my_hair)              |        | print more like MARCO not func                  |
| %r                                                                      |        | raw format, string store with ''                |
| print "." * 10                                                          |        | ..........                                      |
|                                                                         |        | every print do \r\n, unless ',' at last         |
| print """ multi lines """                                               |        | %% for %                                        |
| age = raw_input("How old are you? ")                                    |        |                                                 |
| from sys import argv                                                    | **argv | from argv0                                      |
| txt = open(filename, 'w') \  txt.read() \ write() \ close()             |        |                                                 |
| ... truncate() \ readline() \ len() \ seek()                            |        |                                                 |
| from os.path import exists \ exists(file)                               |        |                                                 |
| def print(*args): \  arg1, arg2 = args \  print "%r, %r" % (arg1, arg2) |        | func body must with blank ahead                 |
| if cond: elif cond: else:                                               |        |                                                 |
| for val in range(x,y): \ while cond:                                    |        |                                                 |
| keywords                                                                |        | http://learnpythonthehardway.org/book/ex37.html |
| print '#'.join(stuff[3:7])                                              |        |                                                 |

* DONE LEARN scrapy : DONE FAQ
  CLOSED: [2015-09-01 Tue 14:47]
scrapy tutorial: http://doc.scrapy.org/en/latest/intro/tutorial.html

XPath expression:
learn more: http://zvon.org/comp/r/tut-XPath_1.html, http://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/
//div[@class="mine"] : selects all div elements which contain an attribute class="mine"
response.xpath('//ul/li/a/@href').extract() : return all link in body
scrapy crawl dmoz -o items.json : save into file

** scrapy command
| create new project              | scrapy startproject <project_name>             | Global  |
| create new splider              | scrapy genspider [-t template] <name> <domain> | Project |
| run splider                     | scrapy crawl <spider>                          | Project |
| get url                         | scrapy fetch <url>                             | Global  |
| open browser to view            | scrapy view <url>                              | Global  |
| get url and analysis by splider | scrapy parse <url> [options]                   | Project |

** splider
the start of splider start_requests(): \  make_requests_from_url(start_urls)

useful command :
response.xpath('//a[contains(@href, "image")]/@href').extract()
divs = response.xpath('//div') \ for p in divs.xpath('.//p'): \ ... print p.extract()    // .// means inside //div, or use 'p'
sel.xpath('//li[re:test(@class, "item-\d$")]//@href').extract()   // with regular
sel = Selector(text=doc, type="html")   // doc is string, for extract more
https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py : example
http://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned : avoid banned
https://www.91ri.org/11469.html : some info about splider

* DONE LEARN XPath
  CLOSED: [2015-09-01 Tue 23:59]
http://zvon.org/xxl/XPathTutorial/General/examples.html

| /                                                                             | absolute path                 |                              |
| //                                                                            | from any level                |                              |
| /*/level                                                                      | * : any name                  |                              |
| /AAA/BBB[last()]                                                              |                               |                              |
| //BBB[@id]                                                                    |                               | <BBB id = "xxx"/>            |
| //BBB[not(@*)]                                                                | without attr                  | <BBB />                      |
| //BBB[@name='bbb']                                                            |                               | <BBB name = "bbb"/>          |
| //BBB[normalize-space(@name)='bbb']                                           | "bbb" is ok too               | <BBB name = " bbb "/>        |
| //*[count(*)=2]                                                               | anyone with two child         | <DDD> <BBB /> <BBB /> </DDD> |
| //*[name()='BBB']                                                             | name() is func                | <BBB />    <BBB id = "xxx"/> |
| //*[starts-with(name(),'B')]                                                  |                               | <BBB />    <BBC />           |
| //*[string-length(name()) &lt 3]                                              |                               | <Q />      <BB />            |
| /AAA/EEE [or] //BBB                                                           | normal [or]                   |                              |
| /child::AAA/child::BBB                                                        | child can be omit             | /AAA/BBB                     |
| //CCC/descendant::DDD                                                         | after any level               |                              |
| //DDD/parent::*                                                               |                               | <EEE> <DDD /> </EEE>         |
| //FFF/ancestor::*                                                             | all level parent              |                              |
| /AAA/BBB/following-sibling::*                                                 | all after that, in same level |                              |
| /AAA/XXX/preceding-sibling::*                                                 | all before it, in same level  |                              |
| //ZZZ/following::*                                                            | all after it, all level       |                              |
| //BBB[position() mod 2 = 0 ]                                                  | start with 1                  |                              |
| //BBB[position() = floor(last() div 2) or position() = ceiling(last() div 2)] |                               |                              |

http://blog.csdn.net/xuandhu/article/details/338934


* TODO CODE Sauro

** try shell 
scrapy shell "http://stock.sohu.com/stock_scrollnews.shtml"
response.xpath('//ul/li/text()').extract()
response.xpath('//div[@class="f14list"]/descendant::a/@href').extract()
response.xpath('//div[@class="main area"]/descendant::div[@class="f14list"]/descendant::a/@href').extract()

to run : scrapy crawl Sauro

** try splash
run splash : sudo docker run -p 8050:8050 scrapinghub/splash

** XML format for output -- Sep. 16 '15
<Sauro Version=1.0 Date=2015-09-16>
  <SauroSite start_url="stock.sohu.com" allow_domain="stock.sohu.com">
    <SauroPage url="stock.sohu.com/onepage.html" title="some title" level=1>    # This is Scrapy.Item, level for crawl deep
      <SauroRefer from="stock.sohu.com" method="javascript:go(next_page)" />
      <SauroStamp signlist="Daaabbb" urlmap="S5-xx" />
      <SauroText mark="base64 of <div class='xx'>" value=3>
        <SauroFragment>
          base64 of content in it
        </SauroFragment>
      </SauroText>
      <SauroText mark="base64 of <div class='yy'>" value=1>
        <SauroFragment>
          base64 of content in it
        </SauroFragment>
      </SauroText>
      <SauroLinks value=10>   # link to text page
        <SauroLink href="link" /> ...
      </SauroLinks>
    </SauroPage>
  </SauroSite>
</Sauro>


* TODO LEARN
Lua: http://www.lua.org/pil/contents.html


Sauropy  Sauropsida
Eublepy  Eublepharis

Sauro

emergency
Sauropsida
Eublepharis
Suereesayas

http://pan.baidu.com/s/1qWn9cSW
https://www.quora.com/Is-there-a-better-crawler-than-Scrapy




http://www.oschina.net/question/269120_133286
http://blog.csdn.net/u012150179/article/details/34913315
  not used http://www.2cto.com/os/201406/311992.html

http://blog.csdn.net/column/details/younghz-scrapy.html many oked
http://blog.csdn.net/zzllabcd/article/details/21380267 easy example for webkit javascript


http://www.v2ex.com/t/135633



http://www.w3.org/TR/xpath-functions/ w3c xpath
http://stackoverflow.com/questions/11744465/xpath-difference-between-node-and-text node() text()
http://stackoverflow.com/questions/14076399/downloader-middleware-to-ignore-all-requests-to-a-certain-url-in-scrapy create downloadmiddleware
http://dh.obdurodon.org/functions.html xpath function

http://blog.csdn.net/heiyeshuwu/article/details/42170017 scrapy + webkit
http://www.jb51.net/article/34156.htm void() href='#'
http://blog.chedushi.com/archives/6488 not ban


http://stock.sohu.com/20150907/n420528997.shtml
http://blog.163.com/yaoyingying681@126/blog/static/109463675201191210280802/ string in python


http://blog.csdn.net/arbel/article/details/7795504 some adv for scrapy, 'SgmlLinkExtractor'

http://my.oschina.net/lpe234/blog/342741 adv scarpy should read
http://blog.csdn.net/iloveyin/article/details/21468641 add more para in python
def parse(self,response):
    yield Request(url, callback=lambda response, typeid=5: self.parse_type(response,typeid))

http://blog.csdn.net/tianmohust/article/details/7621424 dict in python ! very good !

http://www.jb51.net/article/41972.htm except in python
http://www.jb51.net/article/55734.htm  __xxx__ method in python  __slots__ save many memory for huge number of class
http://blog.chinaunix.net/uid-12720249-id-2918322.html  see more for __getitem__

http://www.cnblogs.com/yuxc/archive/2011/08/07/2130229.html for python in language process 

http://wklken.me/ adv for python, as memory, the best index i have see

http://www.cnblogs.com/coser/archive/2011/12/14/2287739.html json myencode mydecode

def parse_type(self,response, typeid):
    print typeid

stampall = dict(stamphave, **stampnot)
stampboth = dict.fromkeys([x for x in stamphave if x in stampnot])

response.xpath('//div[@itemprop="articleBody"]/text()').extract().encode('utf-8')


<div id="root">
  include1
  <div id="useful1">
    include2
      <div id="any">
        include3
      </div>
  </div>
  <div id="useful2">
    include4
    <div id="useless">
      exclude1
      <div id="any">
        exclude2
      </div>
      exclude3
    </div>
  </div>
  include5
</div>


i need 'include1 include2 include3 include4'
the most crazy is how to include1 & 5



* TEST RESULT

const.HOST = 'http://stock.sohu.com/'
const.ALLOW = 'stock.sohu.com
<div itemprop="articleBody"> 256

const.HOST = 'http://finance.sina.com.cn/stock/'
const.ALLOW = 'finance.sina.com.cn'
<div class="article article_16" id="artibody"> 880

const.HOST = 'http://www.stockstar.com/'
const.ALLOW = 'stock.stockstar.com'
<div class="article" id="container-article"> 359

const.HOST = 'http://www.sac.net.cn/'
const.ALLOW = 'www.sac.net.cn'
<div class="Custom_UnionStyle"> 48    

const.HOST = 'http://www.szse.cn/'
const.ALLOW = 'szse.cn
<div class="news_zw"> 16

const.HOST = 'http://money.163.com/stock/'
const.ALLOW = 'money.163.com
<div id="endText" class="end-text"> 949

const.HOST = 'http://finance.people.com.cn/'
const.ALLOW = 'finance.people.com.cn'
<div id="p_content" class="clearfix"> 476

const.HOST = 'http://caijing.chinadaily.com.cn/'
const.ALLOW = 'caijing.chinadaily.com.cn'
<div class="arcBox" id="Zoom"> 303

const.HOST = 'http://finance.ifeng.com/'
const.ALLOW = 'finance.ifeng.com'
<div id="main_content" class="js_selection_area"> 763



sse.com.cn NOT GOOD


        for onesign in response.xpath('//div[@*] | //table[@*] | //form[@*] | //script[@*] | //style[@*]').extract():
            thisname = onesign[1:2]
            if lastname == thisname:
                lastnum += 1
            else:
                if lastnum != 0:
                    totalname += lastname
                    totalname += str(lastnum)
                    lastnum = 0
                lastname = thisname
        print totalname
s6d3s4d1d12d1d46d149d53d8d8d20d8d8d7d9d8d8d8d12d8d11d3d17d6d1d1d3d1d1d4d3s1d69d15d46s3d5

0s6d0s0d3s4d1s0d12f0d0s0d1s0d46s0d149s0d53t0d0t0d8t0d8t0d20t0d8t0d8t0d7s0d9t0d8t0d8t0d8t0d12t0d8t0d11s0d3f0d17t0d0t0d6t0d1t0d1t0d3t0d1t0d1t0d4t0s0d3s1d69s0d15t0d46s3d5


Ms9ds1ds3d2tds1d1s1d12   s2d7sd2td9  s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 8
Ms9ds1ds3d2tds1d1s1d12   s2d7sd2td5  s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 17
Ms9ds1ds3d2tds1d1s1d13   sd18        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 4
Ms9ds1ds3d2tds1d1s1d13   sd18        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 24
Ms9ds1ds3d2tds1d1s1d13   sd15        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 10
Ms9ds1ds3d2tds1d1s1d13   sd8sd4      s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 2
Ms9ds1ds3d2tds1d1s1d13   sd13        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 32
Ms9ds1ds3d2tds1d1s1d13   sd9td4      s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 20
Ms9ds1ds3d2tds1d1s1d13   sd14        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 13
Ms9ds1ds3d2tds1d1s1d13   sd10sd4     s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 8
Ms9ds1ds3d2tds1d1s1d13   sd10td4     s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61      tdsd18sdsd4s3d2s12ds 8
Ms8ds1ds3d2tds1d1s1d13   sd9tdtd5    s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 9
Ms9ds1ds3d2tds1d1s1d13   sd9td6      s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  d7sd61     z tdsd18sdsd4s3d2s12ds 4
Ms9ds1ds3d2tds1d1s1d13   sd15        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 11
Ms9ds1ds3d2tds1d1s1d13   sd17        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 69
Ms9ds1ds3d2tds1d1s1d13   sd12sd4     s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2s  ds1d10s1d2  tdsd18sdsd4s3d2s12ds 7
Ms9ds1ds3d2tds1d1s1d13   sd17        s2d1sd1s10ds2d9s1d2fds1dtds2d2tds1d2sds1d10s1d2  tdsd18sdsd4s3d2s12ds 6
Ms9ds1ds3d2tds1d1s1d13   sd13        s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61  tdsd18sdsd4s3d2s12ds 9

<div itemprop="articleBody"> 261
Ms9ds1ds3d2tds1d1s1d12s2d7sd2td9s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 8
Ms9ds1ds3d2tds1d1s1d12s2d7sd2td5s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 17
Ms9ds1ds3d2tds1d1s1d13sd18s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 4
Ms9ds1ds3d2tds1d1s1d13sd18s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 24
Ms9ds1ds3d2tds1d1s1d13sd15s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 10
Ms9ds1ds3d2tds1d1s1d13sd8sd4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 2
Ms9ds1ds3d2tds1d1s1d13sd13s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 32
Ms9ds1ds3d2tds1d1s1d13sd9td4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 20
Ms9ds1ds3d2tds1d1s1d13sd14s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 13
Ms9ds1ds3d2tds1d1s1d13sd10sd4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 8
Ms9ds1ds3d2tds1d1s1d13sd10td4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 8
Ms8ds1ds3d2tds1d1s1d13sd9tdtd5s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 9
Ms9ds1ds3d2tds1d1s1d13sd9td6s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 4
Ms9ds1ds3d2tds1d1s1d13sd15s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 11
Ms9ds1ds3d2tds1d1s1d13sd17s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 69
Ms9ds1ds3d2tds1d1s1d13sd12sd4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 7
Ms9ds1ds3d2tds1d1s1d13sd17s2d1sd1s10ds2d9s1d2fds1dtds2d2tds1d2sds1d10s1d2tdsd18sdsd4s3d2s12ds 6
Ms9ds1ds3d2tds1d1s1d13sd13s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 9
<div class="lefttext"> 61
Ms1d2fd8tdtd2tdtd2tdtd5tdtdtd7t1d2td3td18sd2td14t2d4td10td1td19sd2td19td2td2td1t1sd3tdtd1s7 27
Ms1d2fd8tdtd2tdtd2tdtd5tdtdtd7t1d2td22sd2td14t2d4td10td1td19sd2td19td2td2td1t1sd3tdtd1s7 2
Ms1d2fd8tdtd2tdtd2tdtd5tdtdtd7t1d2td3td18sd2td14t2d4td10td1td19sd2td19td2td6tdtsd3tdtd1s7 27
Ms1d2fd8tdtd2tdtd2tdtd5tdtdtd7t1d2td3td18sd2td14t2d4td10td1td19sd2td19td2td6tdtsd4s7 3
Ms1d2fd8tdtd2tdtd2tdtd5tdtdtd7t1d2td3td18sd2td14t2d4td10td1td19sd2td19td2td2td1t1sd4s7 2
<div class="text clear" id="contentText"> 26
Ms6ds1ds3d2tds1d1s1d22s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12ds 26
<div class="leftcon"> 19
Ms1d32sd86sdsds3 5
Ms2d14tdtd23s1d91sdsds3 14
<div align="left"> 11
Ms9ds1ds3d2tds1d1s1d13sd36s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 11
<div class="txt"> 8
Ms5d19s2d8sd23sd1s2d3s1dsd5sds4d4s1d3sds1ds4ds 2
Ms5d19s2d8sd21sds2d3s1dsd5sds4d4s1d3sds1ds4ds 6
<div class="righttext"> 2
Ms4d1sd11sd1sd4sd8sd1s3 1
Ms3d1sd11sd1sd4sd8sd1s3 1
<div class="text-pic"> 1
Ms9ds1ds3d2tds1d1s1d13sd10td4s2d3sd1s10ds2d10s1d2fds1dtds2d2tds1d2sd7sd61tdsd18sdsd4s3d2s12ds 1
<div class="commtext"> 1
Msd36sds3 1
<div class="con-l1"> 1
Ms3d10s2d17sd3s3 1
raymon@ubuntu:~/security/Sauro$ 


java callback
http://blog.csdn.net/allen_zhao_2012/article/details/8056665



!!! <div> 1 http://stock.sohu.com/20150914/n421044804.shtml ????? <div> order is different
<div class=\"rc\"> 1  http://stock.sohu.com/s2014/guirenniao/ Intro for CEO
!! <div class=\"righttext\"> 1 http://stock.sohu.com/s2013/caijin28/ subject
<div id=\"ozoom\" style=\"ZOOM: 100%\"> 1 http://stock.sohu.com/20081021/n260145271.shtml news for 2008
<div style=\"text-align: left;\" align=\"center\"> 1 http://stock.sohu.com/20110811/n316091636.shtml news for 2011
<div class=\"cont\"> 1 http://stock.sohu.com/s2011/adc/index.shtml subject
<div style=\"TEXT-JUSTIFY: inter-ideograph; TEXT-ALIGN: left\"> 1 http://stock.sohu.com/20120208/n334078864.shtml maybe ok
<div class=\"pictext\" id=\"picText\"> 1 http://pic.stock.sohu.com/group-381653.shtml pic and text
<div class=\"cut0 Area\"> 1 http://stock.sohu.com/s2009/stock/ s_subject not use
!!! <div class=\"text-pic\"> 1 http://stock.sohu.com/20150915/n421181505.shtml spec format
<div class=\"about\"> 2 http://stock.sohu.com/s2012/aibisen/ subject
<div class=\"box\"> 2 http://q.stock.sohu.com/mac/macdata.shtml macro
NoDiv 2 http://q.stock.sohu.com/cn,gg,300437,2062856328.shtml sohu error
<div class=\"text1 area\" style=\"padding-top:110px\"> 3 http://stock.sohu.com/s2013/xiaogangshinian/ pic and text
<div class=\"mod01\"> 3 http://stock.sohu.com/s2012/haofengchuangyuan/ subject
<div class=\"content clear\" id=\"contentText\"> 5 http://stock.sohu.com/20090623/n264718719.shtml news for 2009
<div align=\"center\"> 6 http://q.stock.sohu.com/news/cn/919/601919/4497377.shtml maybe ok
<div style=\"MARGIN: 0px auto; WIDTH: 950px\" class=\"text1 area\"> 7 http://stock.sohu.com/s2013/shouru/ pic and text
<div class=\"content_text\"> 12 http://stock.sohu.com/s2013/iporeform/ subject
!! <div class=\"leftcon\"> 28 http://stock.sohu.com/s2014/6530/s396618558/ http://stock.sohu.com/s2014/6530/s395940153/ subject
<div class=\"txt\"> 29 http://pic.stock.sohu.com/group-495195.shtml pic and text
<div id=\"content\"> 37 http://stock.sohu.com/s2013/iporeform/ subject
<div class=\"BIZ_itemA_content\"> 47 http://q.stock.sohu.com/cn/002500/jyqk.shtml business
----------------------------------------------------------------------------------------------------
<div align=\"left\"> 76 http://stock.sohu.com/20150922/n421857557.shtml maybe ok
<div id=\"sohu_content\" class=\"article\"> 135 http://stock.sohu.com/20081105/n260446186.shtml news for 2008
<div class=\"text clear\" id=\"contentText\"> 274 http://stock.sohu.com/20120604/n344725212.shtml news for 2012
<div class=\"text clear\" id=\"contentText\" collection=\"Y\"> 358 http://stock.sohu.com/20111027/n323640673.shtml news for 2011
----------------------------------------------------------------------------------------------------
<div class=\"news_content\"> 668 http://q.stock.sohu.com/news/cn/500/002500/4345153.shtml bulletin
<div class=\"BIZ_itemB_content\"> 703 http://q.stock.sohu.com/cn/300480/yjyg.shtml bulletin
<div itemprop=\"articleBody\"> 1476
<div class=\"news_content content\"> 2963
<div class=\"part\"> 8380 http://q.stock.sohu.com/cn,gg,601318,2065884227.shtml bulletin

{
u'<div itemprop="articleBody">': [u'ds1ds3d2tds1d1s1d1', u'sd1s10ds2d', u's1d2fds1dtds2d2tds1d2sd', u'tdsd18sdsd4s3d2s12dsM'], u'<div class="txt">': [u'Ms5d19s2d8sd2', u's2d3s1dsd5sds4d4s1d3sds1ds4dsM'], 
u'<div class="news_content content">': [u'Msd3fd8tdtd2tdtd2tdtd4tdtdtd6t', u'd2td3td1'], 
u'<div class="BIZ_itemB_content">': [u'Ms6d3sd2sd3fd6t1d'], 
u'<div class="text clear" id="contentText">': [u'Ms6ds1ds3d2tds1d1s1d',                                                                 u's4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM'], 
u'<div class="part">': [u'Ms6d3sd2sd3fd']}


{u'<div itemprop="articleBody">': [u'ds1ds3d2tds1d1s1d1', u's1d2fds1dtds2d2tds1d2sd', u'tdsd18sdsd4s3d2s12dsM'], u'<div class="BIZ_itemB_content">': [u'Ms6d3sd2sd3fd6t1d'], u'<div class="txt">': [u's2d3s1dsd5sds4d4s1d3sds1ds4dsM'], u'<div class="news_content content">': [u'Msd3fd8tdtd2tdtd2tdtd4tdtdtd6t'], u'<div class="text clear" id="contentText">': [u'Ms6ds1ds3d2tds1d1s1d', u's4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM']}
PART match <div itemprop="articleBody"> Ms6ds1ds3d2tds1d1s1d16sdtd4s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM tdsd18sdsd4s3d2s12dsM
PART match <div itemprop="articleBody"> Ms6ds1ds3d2tds1d1s1d17td6s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM tdsd18sdsd4s3d2s12dsM
PART match <div itemprop="articleBody"> Ms6ds1ds3d2tds1d1s1d16td4s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM tdsd18sdsd4s3d2s12dsM
PART match <div itemprop="articleBody"> Ms6ds1ds3d2tds1d1s1d26s4ds3ds2d6s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s3d2s12dsM tdsd18sdsd4s3d2s12dsM
PART match <div itemprop="articleBody"> Ms9ds1ds3d2tds1d1s1d12s2d7sd2tdtdtdtdtd6s2d2sd1s10ds2d10s1d2fds1dtds2d28tds1d2sd7sd61tdsd18sdsd4s3d2s12dsM tdsd18sdsd4s3d2s12dsM
PART match <div class="text clear" id="contentText"> Ms6ds1ds3d2tds1d1s1d21s5ds3ds2d7s1d2fds1dtds2d28tds1d2sd7sd61tdsd18sdsd4s5d2s12dsM s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM
PART match <div class="text clear" id="contentText"> Ms6ds1ds3d2tds1d1s1d26s4ds3ds2d6s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s3d2s12dsM s4ds3ds2d7s1d2fds1dtds2d28tds1d2sd5sd20sd61tdsd18sdsd4s5d2s12dsM


@2.获取文件夹大小，即遍历文件夹，将所有文件大小加和。遍历文件夹使用os.walk函数
import os  
from os.path import join, getsize  
  
def getdirsize(dir):  
   size = 0L  
   for root, dirs, files in os.walk(dir):  
      size += sum([getsize(join(root, name)) for name in files])  
   return size 
   
   
   
   
   
   

http://ergoemacs.org/emacs/emacs_copy_cut_current_line.html config emacs
